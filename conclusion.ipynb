{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0643c26",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c07269",
   "metadata": {},
   "source": [
    "## **Understanding the Environment and its Rewards**\n",
    "\n",
    "The environment is a 2D grid world where the agent can move in four directions: up, down, left, and right. There are 3 different types of tiles: \n",
    "- **Empty tiles**: The agent can move freely on these tiles.\n",
    "- **Wall tiles**: The agent cannot move through these tiles.\n",
    "- **Goal tile**: The agent receives a reward of +1 when it reaches this tile.\n",
    "\n",
    "The agent starts at a random position on the grid and must visit all goal tiles, making this a CPP (Coverage Path Planning) problem. The agent receives a reward of +2 for each goal tile it visits and a penalty of -1 for each step it takes to encourage shorter paths.\n",
    "\n",
    "#### **Reward function:**\n",
    "\n",
    "```python\n",
    "        # Default baseline reward\n",
    "        reward = -1\n",
    "\n",
    "        # Check validity and apply penalties\n",
    "        if not (0 <= ni < self.H and 0 <= nj < self.W and self.grid[ni, nj] == 0):\n",
    "            # Invalid action: stay in place\n",
    "            self.agent_pos = (i, j) \n",
    "        else:\n",
    "            # Valid move: update position\n",
    "            self.agent_pos = (ni, nj)\n",
    "\n",
    "        # Check if on a target\n",
    "        if self.agent_pos not in self.visited:\n",
    "            if self.agent_pos in self.targets:\n",
    "                reward = 2   # new target\n",
    "            self.visited.add(self.agent_pos)\n",
    "```"
   ]
  },
  {
   "attachments": {
    "level2_map.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAIAAAAErfB6AAABsElEQVR4nO3asZHCQBBFweVCIUeiIEdS0SVwhkpcFTuPbv8b0jPWmbVIu621juO4srzZDtj+XJgxiMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCcwwLZG3hnZnt96g+MEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4gQG2NfLO6MLqv4z7V97gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE5ggG19403WxO91k8XfBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIEBtvXurdD9eb+wfT1eH9yOu6t6Z+sNjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEBtiWm6yz3GSxI4HjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE5ggG25yTrLTRY7EjhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wQG2NbIO6Oh24/wBscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAsNkv+N+shndliX7AAAAAElFTkSuQmCC"
    },
    "level3_map.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAIAAAAErfB6AAABuElEQVR4nO3dsY3DQBAEQVKhKMePQjkqFX4KB0qAbhtV/jhs45wFeBykncdxPF/PG8v33/vD7a9c13VjdZ7nxO3jxoxBBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIEBtjX1JmvcbdSN1Vd4g+MEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4gQG29cubrE+2brIWeYPjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIEBtjXyf3y261tvcJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwnMMC2Rt4Z2a5vvcFxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMAA2xp5Z2S7vvUGxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECA2xr5J3RjdW3jPtW3uA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TmCAbU29ybJd3HqD4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiBYbJ/tuSmGbQ1/G4AAAAASUVORK5CYII="
    },
    "level4_map.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAIAAAAErfB6AAABvElEQVR4nO3dsW3DMBRFUTqjeMdMkR29irKACkEKQPLmnP4Vxi3YfMhjkPYaYxzHcWf5erqdZdbvnbL9ujFjIwLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAAMv6jzdZ75/3jdXn+zNx6yaLcwLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAAMva8ttPD7ez7qqecJPFOYHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE5ggGXN/NaVm6zr3GRxTuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhMYYFm7/v/gjt/nmrL1BscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAgMsa8s7I9vrW29wnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCcwwLK2vDN6uJ3FTRZ/T+A4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhMYYFm73lXZXtx6g+MEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4gWFnv8uUoBlLpZfcAAAAAElFTkSuQmCC"
    },
    "level5_map.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAIAAAAErfB6AAABwElEQVR4nO3dsW3DQBQFQcqlqEdX4R7VCpU6cEDcGeDdaib/oIQNmDxIx0Ha4ziO8zxHLh+73t7llu/7NXDGRgSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiBAZY1u1GyybrOJov/J3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkMsKyP2yhtejvMOzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOExhgWbtulNxevPUOjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEjhM4TuA4geMEBljWlr/9NPncj/rM3sFxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMAAy5rdKD1/ngO3r+/XwNVvM8+1yaJD4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOExhgWTZZV9lksSKB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhOYIBlze6MZrZRd+25Ztz1n4k2WfxN4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOExh29gYhAY4ZzIUaygAAAABJRU5ErkJggg=="
    },
    "level_map_bordered.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAIAAAAErfB6AAABrklEQVR4nO3csXEDMRAEQVChMEdFoRyZyisBGah/Q7ipbn+tMeBcYS3SXmut67ruLF+2A7ZfN2YMInCcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkMcKyRd0a2+1tvcJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwnMMCxRt4ZPdy+f943tp/vzz9u3WTxN4HjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE5ggGNNvat6sn3CTRZnEThO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wSOEzhO4DiB4wQGONbUuyr/ZO1wk9UncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCQxwrKl3VbabW29wnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCdwnMBxAscJHCcwwLFG3hnZ7m+9wXECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwADHGnlnZLu/9QbHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLDZL/VE74ZLs2MrAAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "9ed3e31e",
   "metadata": {},
   "source": [
    "## **Levels**\n",
    "\n",
    "The environment consists of 5 levels, all with the same grid size but with a changing number of goal tiles and walls. The levels are designed to increase in difficulty, with the first level being the easiest and the last level being the hardest. The levels are as follows, with the target tiles represented in green, the walls in black, and the empty tiles in white:\n",
    "\n",
    "**Level 1**:\n",
    "\n",
    "![level1_map.png](attachment:level_map_bordered.png)\n",
    "\n",
    "**Level 2**:\n",
    "\n",
    "![level2_map.png](attachment:level2_map.png)\n",
    "\n",
    "**Level 3**:\n",
    "\n",
    "![level3_map.png](attachment:level3_map.png)\n",
    "\n",
    "**Level 4**:\n",
    "\n",
    "![level4_map.png](attachment:level4_map.png)\n",
    "\n",
    "**Level 5**:\n",
    "\n",
    "![level5_map.png](attachment:level5_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43c830",
   "metadata": {},
   "source": [
    "## **Baseline A\\* Algorithm**\n",
    "\n",
    "The A* algorithm is a well-known pathfinding algorithm that finds the shortest path from the start node to the goal node. It uses a heuristic function to estimate the cost of reaching the goal from the current node. The A* algorithm is used as a baseline to compare the performance of the reinforcement learning models.\n",
    "\n",
    "**a_star_test.ipynb**\n",
    "\n",
    "## **Models Used**\n",
    "\n",
    "### **DQN (Deep Q-Network)**\n",
    "\n",
    "The DQN algorithm is a reinforcement learning algorithm that uses a neural network to approximate the Q-value function. The Q-value function is used to estimate the expected future rewards for each action taken in a given state. The DQN algorithm uses experience replay and target networks to stabilize training and improve performance.\n",
    "\n",
    "**dqn_mlp_test.ipynb** \n",
    "\n",
    "### **PPO (Proximal Policy Optimization) w/o L2 Regularization**\n",
    "\n",
    "PPO is a policy gradient method that optimizes the policy directly. It uses a clipped objective function to ensure that the policy update does not deviate too much from the previous policy, which helps to stabilize training.\n",
    "\n",
    "**ppo_mlp_test.ipynb**\n",
    "\n",
    "### **PPO (Proximal Policy Optimization) w/ L2 Regularization**\n",
    "\n",
    "This is the same as the previous PPO model but with L2 regularization added to the loss function. L2 regularization helps to prevent overfitting by adding a penalty for large weights in the model.\n",
    "\n",
    "**ppo_mlp_l2_test.ipynb**\n",
    "\n",
    "## **Policy Used**\n",
    "\n",
    "### **Mlp Policy**\n",
    "\n",
    "The MlpPolicy is a multi-layer perceptron (MLP) policy that uses fully connected layers to process the input state and output the action probabilities. The MlpPolicy is used in both the DQN and PPO models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0daf62",
   "metadata": {},
   "source": [
    "## **Working Agent Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ab9b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 13:08:20.465 Python[53502:4448997] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-05-10 13:08:20.465 Python[53502:4448997] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renderer closed, notebook run complete.\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from stable_baselines3 import DQN\n",
    "from coverage_env import CoverageEnv\n",
    "from pygame_renderer import PygameRenderer\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "env = CoverageEnv(curriculum=1)\n",
    "model = DQN.load(\"models/dqn/mlp/coverage_lvl1.zip\", env=env)\n",
    "renderer = PygameRenderer(env, tile_size=50)\n",
    "\n",
    "obs, _info = env.reset()\n",
    "done = False\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                raise KeyboardInterrupt()\n",
    "\n",
    "        # predict returns an array; convert to int\n",
    "        raw_action, _ = model.predict(obs, deterministic=True)\n",
    "        action = int(raw_action)  \n",
    "\n",
    "        obs, reward, terminated, truncated, _info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        renderer.render()\n",
    "\n",
    "        if done:\n",
    "            obs, _info = env.reset()\n",
    "            done = False\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    renderer.close()\n",
    "    print(\"Renderer closed, notebook run complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
