{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model per Level**\n",
    "\n",
    "The goal of this notebook is to train one model instance of our best model (PPO with L2) for each environment without curriculum learning, to understand if it really learns faster with exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from coverage_env import CoverageEnv\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_0 = CoverageEnv(curriculum=0)\n",
    "env_1 = CoverageEnv(curriculum=1)\n",
    "env_2 = CoverageEnv(curriculum=2)\n",
    "env_3 = CoverageEnv(curriculum=3)\n",
    "env_4 = CoverageEnv(curriculum=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    optimizer_class  = AdamW,\n",
    "    optimizer_kwargs = dict(\n",
    "        weight_decay = 1e-4\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Environment 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env_0,\n",
    "    learning_rate = 1e-4,\n",
    "    n_steps       = 2048,\n",
    "    batch_size    = 64,\n",
    "    n_epochs      = 10,\n",
    "    gamma         = 0.99,\n",
    "    gae_lambda    = 0.95,\n",
    "    clip_range    = 0.2,\n",
    "    ent_coef      = 0.01,\n",
    "    vf_coef       = 0.5,\n",
    "    max_grad_norm = 0.5,\n",
    "    verbose       = 1,\n",
    "    policy_kwargs = policy_kwargs,\n",
    "    tensorboard_log=\"logs/general/coverage_lvl0\",\n",
    ")\n",
    "\n",
    "# train for 500k timesteps\n",
    "model.learn(total_timesteps=500_000)\n",
    "\n",
    "# save it\n",
    "model.save(\"models/general/coverage_lvl0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/general/coverage_lvl0/PPO_1\"\n",
    "run_id = os.listdir(log_dir)[0]\n",
    "event_path = os.path.join(log_dir, run_id)\n",
    "\n",
    "# Load the TensorBoard logs\n",
    "event_acc = EventAccumulator(event_path)\n",
    "event_acc.Reload()\n",
    "\n",
    "# Get 'rollout/ep_rew_mean' scalar events\n",
    "rewards = event_acc.Scalars(\"rollout/ep_rew_mean\")\n",
    "\n",
    "# Extract steps and reward values\n",
    "steps = [event.step for event in rewards]\n",
    "reward_values = [event.value for event in rewards]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, reward_values, label=\"Mean Episode Reward\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Learning Curve without Curriculum (Level 0)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    env_0,\n",
    "    n_eval_episodes=20,\n",
    "    deterministic=True,\n",
    ")\n",
    "print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Environment 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env_1,\n",
    "    learning_rate = 1e-4,\n",
    "    n_steps       = 2048,\n",
    "    batch_size    = 64,\n",
    "    n_epochs      = 10,\n",
    "    gamma         = 0.99,\n",
    "    gae_lambda    = 0.95,\n",
    "    clip_range    = 0.2,\n",
    "    ent_coef      = 0.01,\n",
    "    vf_coef       = 0.5,\n",
    "    max_grad_norm = 0.5,\n",
    "    verbose       = 1,\n",
    "    policy_kwargs = policy_kwargs,\n",
    "    tensorboard_log=\"logs/general/coverage_lvl1\",\n",
    ")\n",
    "\n",
    "# train for 500k timesteps\n",
    "model.learn(total_timesteps=500_000)\n",
    "\n",
    "# save it\n",
    "model.save(\"models/general/coverage_lvl1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/general/coverage_lvl1/PPO_1\"\n",
    "run_id = os.listdir(log_dir)[0]\n",
    "event_path = os.path.join(log_dir, run_id)\n",
    "\n",
    "# Load the TensorBoard logs\n",
    "event_acc = EventAccumulator(event_path)\n",
    "event_acc.Reload()\n",
    "\n",
    "# Get 'rollout/ep_rew_mean' scalar events\n",
    "rewards = event_acc.Scalars(\"rollout/ep_rew_mean\")\n",
    "\n",
    "# Extract steps and reward values\n",
    "steps = [event.step for event in rewards]\n",
    "reward_values = [event.value for event in rewards]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, reward_values, label=\"Mean Episode Reward\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Learning Curve without Curriculum (Level 1)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    env_1,\n",
    "    n_eval_episodes=20,\n",
    "    deterministic=True,\n",
    ")\n",
    "print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Environment 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env_2,\n",
    "    learning_rate = 1e-4,\n",
    "    n_steps       = 2048,\n",
    "    batch_size    = 64,\n",
    "    n_epochs      = 10,\n",
    "    gamma         = 0.99,\n",
    "    gae_lambda    = 0.95,\n",
    "    clip_range    = 0.2,\n",
    "    ent_coef      = 0.01,\n",
    "    vf_coef       = 0.5,\n",
    "    max_grad_norm = 0.5,\n",
    "    verbose       = 1,\n",
    "    policy_kwargs = policy_kwargs,\n",
    "    tensorboard_log=\"logs/general/coverage_lvl2\",\n",
    ")\n",
    "\n",
    "# train for 500k timesteps\n",
    "model.learn(total_timesteps=500_000)\n",
    "\n",
    "# save it\n",
    "model.save(\"models/general/coverage_lvl2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/general/coverage_lvl2/PPO_1\"\n",
    "run_id = os.listdir(log_dir)[0]\n",
    "event_path = os.path.join(log_dir, run_id)\n",
    "\n",
    "# Load the TensorBoard logs\n",
    "event_acc = EventAccumulator(event_path)\n",
    "event_acc.Reload()\n",
    "\n",
    "# Get 'rollout/ep_rew_mean' scalar events\n",
    "rewards = event_acc.Scalars(\"rollout/ep_rew_mean\")\n",
    "\n",
    "# Extract steps and reward values\n",
    "steps = [event.step for event in rewards]\n",
    "reward_values = [event.value for event in rewards]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, reward_values, label=\"Mean Episode Reward\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Learning Curve without Curriculum (Level 2)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    env_2,\n",
    "    n_eval_episodes=20,\n",
    "    deterministic=True,\n",
    ")\n",
    "print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Environment 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env_3,\n",
    "    learning_rate = 1e-4,\n",
    "    n_steps       = 2048,\n",
    "    batch_size    = 64,\n",
    "    n_epochs      = 10,\n",
    "    gamma         = 0.99,\n",
    "    gae_lambda    = 0.95,\n",
    "    clip_range    = 0.2,\n",
    "    ent_coef      = 0.01,\n",
    "    vf_coef       = 0.5,\n",
    "    max_grad_norm = 0.5,\n",
    "    verbose       = 1,\n",
    "    policy_kwargs = policy_kwargs,\n",
    "    tensorboard_log=\"logs/general/coverage_lvl3\",\n",
    ")\n",
    "\n",
    "# train for 500k timesteps\n",
    "model.learn(total_timesteps=500_000)\n",
    "\n",
    "# save it\n",
    "model.save(\"models/general/coverage_lvl3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/general/coverage_lvl3/PPO_1\"\n",
    "run_id = os.listdir(log_dir)[0]\n",
    "event_path = os.path.join(log_dir, run_id)\n",
    "\n",
    "# Load the TensorBoard logs\n",
    "event_acc = EventAccumulator(event_path)\n",
    "event_acc.Reload()\n",
    "\n",
    "# Get 'rollout/ep_rew_mean' scalar events\n",
    "rewards = event_acc.Scalars(\"rollout/ep_rew_mean\")\n",
    "\n",
    "# Extract steps and reward values\n",
    "steps = [event.step for event in rewards]\n",
    "reward_values = [event.value for event in rewards]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, reward_values, label=\"Mean Episode Reward\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Learning Curve without Curriculum (Level 3)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    env_3,\n",
    "    n_eval_episodes=20,\n",
    "    deterministic=True,\n",
    ")\n",
    "print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Environment 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env_4,\n",
    "    learning_rate = 1e-4,\n",
    "    n_steps       = 2048,\n",
    "    batch_size    = 64,\n",
    "    n_epochs      = 10,\n",
    "    gamma         = 0.99,\n",
    "    gae_lambda    = 0.95,\n",
    "    clip_range    = 0.2,\n",
    "    ent_coef      = 0.01,\n",
    "    vf_coef       = 0.5,\n",
    "    max_grad_norm = 0.5,\n",
    "    verbose       = 1,\n",
    "    policy_kwargs = policy_kwargs,\n",
    "    tensorboard_log=\"logs/general/coverage_lvl4\",\n",
    ")\n",
    "\n",
    "# train for 500k timesteps\n",
    "model.learn(total_timesteps=500_000)\n",
    "\n",
    "# save it\n",
    "model.save(\"models/general/coverage_lvl4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/general/coverage_lvl4/PPO_1\"\n",
    "run_id = os.listdir(log_dir)[0]\n",
    "event_path = os.path.join(log_dir, run_id)\n",
    "\n",
    "# Load the TensorBoard logs\n",
    "event_acc = EventAccumulator(event_path)\n",
    "event_acc.Reload()\n",
    "\n",
    "# Get 'rollout/ep_rew_mean' scalar events\n",
    "rewards = event_acc.Scalars(\"rollout/ep_rew_mean\")\n",
    "\n",
    "# Extract steps and reward values\n",
    "steps = [event.step for event in rewards]\n",
    "reward_values = [event.value for event in rewards]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, reward_values, label=\"Mean Episode Reward\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Learning Curve without Curriculum (Level 4)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    env_4,\n",
    "    n_eval_episodes=20,\n",
    "    deterministic=True,\n",
    ")\n",
    "print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-cpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
